# ğŸš€ START HERE - Getting the LLM Working

This guide will get you from zero to a working AI-powered chat in 10 minutes.

## TL;DR - The Fastest Path

```bash
# 1. Install Ollama (if not installed)
# Mac: brew install ollama
# Or download from: https://ollama.com

# 2. Start Ollama
ollama serve

# 3. Pull a model (in new terminal)
ollama pull llama3

# 4. Setup backend
cd backend
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 5. Start backend (keep Ollama running!)
python -m app.main

# 6. Start frontend (in new terminal)
npm install
npm run dev

# 7. Open http://localhost:5173 and test!
```

## Detailed Steps

### Step 1: Install Ollama âš™ï¸

**Mac:**
```bash
brew install ollama
```

**Windows/Linux:**
- Download from: https://ollama.com/download
- Run installer

**Verify:**
```bash
ollama --version
```

ğŸ“– **Need help?** See [INSTALL_OLLAMA.md](./INSTALL_OLLAMA.md)

### Step 2: Start Ollama ğŸŸ¢

```bash
ollama serve
```

**Keep this terminal open!** Ollama needs to keep running.

### Step 3: Download a Model ğŸ“¦

Open a **new terminal** and run:

```bash
# Choose one (llama3 recommended):
ollama pull llama3    # Best quality (~4.7GB)
# OR
ollama pull mistral   # Faster (~4GB)
# OR  
ollama pull phi3      # Fastest, smallest (~2GB)
```

This downloads the model (takes 2-5 minutes).

**Verify:**
```bash
ollama list
# Should show your model
```

**Test:**
```bash
ollama run llama3
# Type: "Hello"
# Should get response. Type /bye to exit
```

### Step 4: Setup Backend ğŸ

```bash
cd backend

# Create virtual environment
python3 -m venv venv

# Activate it
source venv/bin/activate  # Mac/Linux
# OR
venv\Scripts\activate     # Windows

# Install dependencies
pip install -r requirements.txt
```

**Check setup:**
```bash
python check_setup.py
```

This will verify everything is ready.

### Step 5: Start Backend ğŸš€

**Make sure Ollama is still running** (Step 2), then:

```bash
# In backend/ directory with venv activated
python -m app.main
```

You should see:
```
âœ“ Connected to Ollama (llama3)
âœ“ Loaded house manual for prop-1...
âœ“ Backend initialized successfully!
INFO: Uvicorn running on http://0.0.0.0:8000
```

**Keep this terminal open!**

### Step 6: Start Frontend ğŸ’»

Open a **new terminal**:

```bash
# In project root
npm install  # If first time
npm run dev
```

You should see:
```
VITE ready in XXX ms
âœ  Local:   http://localhost:5173/
```

### Step 7: Test It! ğŸ‰

1. Open http://localhost:5173
2. Login: `tenant@example.com` / `admin123`
3. Go to **Messages**
4. Type: **"How does the TV work?"**
5. Watch the AI respond automatically! ğŸ¤–

## What You Should Have Running

**3 terminal windows:**

1. **Terminal 1**: `ollama serve` (Ollama server)
2. **Terminal 2**: `python -m app.main` (Backend API)
3. **Terminal 3**: `npm run dev` (Frontend)

## Troubleshooting

### âŒ "ollama: command not found"
â†’ Install Ollama first (Step 1)

### âŒ "Could not connect to Ollama"
â†’ Make sure `ollama serve` is running (Step 2)

### âŒ "No models found"
â†’ Pull a model: `ollama pull llama3` (Step 3)

### âŒ "Module not found" (Python)
â†’ Activate venv and install: `pip install -r requirements.txt`

### âŒ Frontend can't connect
â†’ Check backend is running on port 8000
â†’ Check browser console for errors

### âŒ Slow responses
â†’ First request loads models (10-30 seconds) - this is normal!
â†’ Subsequent requests are faster (2-5 seconds)
â†’ Use `phi3` model for faster responses

## Quick Health Checks

```bash
# Check Ollama
ollama list
curl http://localhost:11434/api/tags

# Check backend
curl http://localhost:8000/health

# Check what's running
lsof -i :8000   # Backend
lsof -i :5173   # Frontend  
lsof -i :11434  # Ollama
```

## Next Steps

Once it's working:
- âœ… Try different questions
- âœ… Test issue reporting ("The AC is making noise")
- âœ… Check backend logs for AI responses
- âœ… Customize house manuals in `backend/data/house_manuals/`

## Need More Help?

- **Installation issues?** â†’ [INSTALL_OLLAMA.md](./INSTALL_OLLAMA.md)
- **Detailed setup?** â†’ [QUICK_START.md](./QUICK_START.md)
- **Backend docs?** â†’ [backend/README.md](./backend/README.md)

---

**You're all set! ğŸ‰** The AI will now automatically respond to tenant messages.

